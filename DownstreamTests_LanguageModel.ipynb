{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XrvDAISh50tq"
   },
   "source": [
    "## CS287r Final Project\n",
    "## Language Model Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MtyuJBXJd_h3"
   },
   "source": [
    "## Install Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "qrtURpMz5wy2",
    "outputId": "0bf6acc8-3743-4a17-d27e-19f039dcc801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biopython\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/15/8ac646ff24cfa2588b4d5e5ea51e8d13f3d35806bd9498fbf40ef79026fd/biopython-1.73-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2MB 5.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from biopython) (1.16.3)\n",
      "Installing collected packages: biopython\n",
      "Successfully installed biopython-1.73\n",
      "\u001b[K     |████████████████████████████████| 61kB 3.9MB/s \n",
      "\u001b[?25h  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for namedtensor (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# !pip install biopython\n",
    "# !pip install -q torch torchtext opt_einsum\n",
    "# !pip install -qU git+https://github.com/harvardnlp/namedtensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "Dfv_2I-3-j3o",
    "outputId": "f2555dd6-49d6-43b6-c973-b92e47377bbc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-2bdaaf8b-ed6c-40ba-9a1d-ea350e3a9a8d\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-2bdaaf8b-ed6c-40ba-9a1d-ea350e3a9a8d\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ecoli_CDS.fasta to ecoli_CDS.fasta\n"
     ]
    }
   ],
   "source": [
    "## Upload cds file..\n",
    "\n",
    "#!wget -m ftp://ftp.ensemblgenomes.org/pub/release-43/bacteria//fasta/bacteria_0_collection/escherichia_coli_str_k_12_substr_mg1655/cdna/\n",
    "#!mv ftp.ensemblgenomes.org/pub/release-43/bacteria/fasta/bacteria_0_collection/escherichia_coli_str_k_12_substr_mg1655/cdna/Escherichia_coli_str_k_12_substr_mg1655.ASM584v2.cdna.all.fa.gz ./ \n",
    "#!unzip \"Escherichia_coli_str_k_12_substr_mg1655.ASM584v2.cdna.all.fa.gz\"\n",
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_nJPkWqsys-u",
    "outputId": "699eaf4a-371a-4597-c0eb-2bf445ecfd3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat 'Escherichia_coli_str_k_12_substr_mg1655.ASM584v2.cdna.all.fa': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "mv Escherichia_coli_str_k_12_substr_mg1655.ASM584v2.cdna.all.fa ecoli_CDS.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EY3JqLxk50HJ"
   },
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqUtils import CodonUsage\n",
    "from Bio.SeqUtils import IUPACData\n",
    "import torch\n",
    "import torchtext\n",
    "from namedtensor import ntorch\n",
    "from namedtensor.text import NamedField\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.util import ngrams\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "# from google.colab import files\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "biBt0uYHeGU1"
   },
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L53cBDAB50JR"
   },
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "# Prepend with a start token\n",
    "tokenize = lambda x : [\"<START>\"] + re.findall('.{%d}' % 3, x)\n",
    "TEXT = NamedField(names=(\"seqlen\", ), sequential=True, \n",
    "                  lower=True, tokenize=tokenize)\n",
    "AA_LABEL = NamedField(names=(\"seqlen\", ), \n",
    "                  lower=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yrQxUwLn50Lk"
   },
   "outputs": [],
   "source": [
    "# Note: Fairly round about way of getting this loaded into pytorch..\n",
    "# Wrangling data into format pytorch language prefers.. \n",
    "sequences = [str(rec.seq) for rec in SeqIO.parse(\"sequences/ecoli_CDS.fasta\", \"fasta\") if len(rec.seq) % 3 == 0]\n",
    "df = pd.DataFrame(sequences, columns=[\"sequence\"])\n",
    "df.to_csv(\"cds.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "XT72gjMkR5bj",
    "outputId": "1596470d-c1ee-4ccc-d2bf-f4d2f3170b3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START>',\n",
       " 'GTG',\n",
       " 'TCA',\n",
       " 'CTT',\n",
       " 'TCG',\n",
       " 'CTT',\n",
       " 'TGG',\n",
       " 'CAG',\n",
       " 'CAG',\n",
       " 'TGT',\n",
       " 'CTT',\n",
       " 'GCC',\n",
       " 'CGA',\n",
       " 'TTG',\n",
       " 'CAG',\n",
       " 'GAT',\n",
       " 'GAG',\n",
       " 'TTA',\n",
       " 'CCA',\n",
       " 'GCC',\n",
       " 'ACA',\n",
       " 'GAA']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_ = [rec.seq for rec in SeqIO.parse(\"ecoli_CDS.fasta\", \"fasta\") if len(rec.seq) % 3 == 0]\n",
    "tokenize(str(sequences_[0][:21*3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NXgf0rBVWDCO"
   },
   "outputs": [],
   "source": [
    "#generate all DNA triplets\n",
    "bases = \"tcag\"\n",
    "codons = [a + b + c for a in bases for b in bases for c in bases]\n",
    "aa = [str(Seq(j).translate()) for j in codons]\n",
    "codon_to_aa = dict(zip(codons, aa))\n",
    "\n",
    "\n",
    "#create one hot of AA encoding\n",
    "AA_LABEL.build_vocab(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-p--2rJY50OE"
   },
   "outputs": [],
   "source": [
    "my_data = torchtext.data.TabularDataset(\"cds.csv\", format=\"CSV\", \n",
    "                                        fields=[(\"sequence\", TEXT)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xiVku0e246Cz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5O0OCHykEUtu"
   },
   "outputs": [],
   "source": [
    "train, test = my_data.split(split_ratio=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5aoF2rKyM6FF"
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oBEAtnKJUjwg"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    " \n",
    "#we don't split genes\n",
    "train_iter_bucket, test_iter_bucket = torchtext.data.BucketIterator.splits(\n",
    "    (train, test), batch_sizes=(10,10), sort_within_batch=False, sort_key=lambda x : len(x.sequence),\n",
    "    device=torch.device(device, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ib9xVCdgGq6"
   },
   "outputs": [],
   "source": [
    "# Make a look up table, such that you can index with the vocab item (e.g. a codon)\n",
    "# and get the one hot corresponding to its amino acid\n",
    "one_hot_vec = torch.eye(len(AA_LABEL.vocab))\n",
    "zero_vec =  torch.zeros(len(AA_LABEL.vocab), 1)\n",
    "direct_look_up = [one_hot_vec[AA_LABEL.vocab.stoi[codon_to_aa[TEXT.vocab.itos[i]]]].unsqueeze(1) \n",
    "                  if TEXT.vocab.itos[i] in codon_to_aa else zero_vec\n",
    "                  for i in range(len(TEXT.vocab.stoi))]\n",
    "\n",
    "# Codon x one hot \n",
    "index_table = torch.cat(direct_look_up, dim=1).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9qbFSWzAzFJF"
   },
   "outputs": [],
   "source": [
    "####\n",
    "\n",
    "# TEXT.vocab.itos[x]: retrieves codon given numbering in the order of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "94Bv7I2y1XuI"
   },
   "source": [
    "\n",
    "TEXT.vocab.itos[num]: retrieves codon given numbering in the encoding position\n",
    "\n",
    "TEXT.vocab.stoi[\"gaa\"]: retrieves encoding given codon\n",
    "\n",
    "codon_to_aa: dictionary. key: codon, val: AA\n",
    "\n",
    "AA_LABEL.vocab.stoi[AA]: retrieves AA encoding given AA\n",
    "AA_LABEL.vocab.itos[num]: retrieves AA given AA encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4s1Vg9grfRV2"
   },
   "outputs": [],
   "source": [
    "codon_to_aa_index = torch.argmax(index_table, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "zugdkAQUEp8k",
    "outputId": "390fbf8c-1de5-4785-8664-ad0565057a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codon_embed = TEXT.vocab.stoi[\"aat\"]\n",
    "print(codon_embed)\n",
    "\n",
    "# One hot of amino acid embedding\n",
    "index_table[codon_embed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfWyJa0MM6rx"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter_bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pVoYFxtCJuy4",
    "outputId": "ad207fca-6f0b-440c-dce3-6d21347ce90d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('batch', 10), ('seqlen', 749), ('onehot', 23)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = ntorch.nn.Embedding.from_pretrained(index_table).spec(\"seqlen\", \"onehot\").to(device)\n",
    "embed(batch.sequence).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WxprVRmL_H0"
   },
   "outputs": [],
   "source": [
    "# Build masking table\n",
    "# Here, if it's a synonymous option, give it 0 value, if not, give -1e9\n",
    "# Add this with the output vector (i.e. output += mask_tbl[trg]) before softmax\n",
    "mask_tbl = torch.tensor(np.array([[0 if (codon in codon_to_aa and codon_2 in codon_to_aa and codon_to_aa[codon] == codon_to_aa[codon_2]) else -1e9 \n",
    "           for codon_2 in TEXT.vocab.itos] \n",
    "          for index, codon in enumerate(TEXT.vocab.itos)])).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "EoNXvzAsGDwp",
    "outputId": "15b2e7b5-f806-4554-8177-022ea8997479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODON aaa aag\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,  0.0000e+00,\n",
       "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "        -1.0000e+09, -1.0000e+09,  0.0000e+00, -1.0000e+09, -1.0000e+09,\n",
       "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
       "        -1.0000e+09, -1.0000e+09], dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"CODON\", TEXT.vocab.itos[4], TEXT.vocab.itos[42])\n",
    "\n",
    "mask_tbl[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eT8A05zpMCQw",
    "outputId": "2d3e2212-2849-4039-e508-8c35adebf09c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2369\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 0\n",
    "for batch in train_iter_bucket:\n",
    "  if batch.sequence.shape[\"seqlen\"] > max_seq_len: \n",
    "    max_seq_len = batch.sequence.shape[\"seqlen\"] \n",
    "\n",
    "for batch in test_iter_bucket:\n",
    "  if batch.sequence.shape[\"seqlen\"] > max_seq_len: \n",
    "    max_seq_len = batch.sequence.shape[\"seqlen\"] \n",
    "\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0qthAXLif_u"
   },
   "source": [
    "## Import Data (Character Level Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XbSKJ2tpiidg"
   },
   "outputs": [],
   "source": [
    "tokenize_char = lambda x : [\"<START>\"] + list(x)\n",
    "TEXT_CHAR = NamedField(names=(\"seqlen\", ), sequential=True, \n",
    "                  lower=True, tokenize=tokenize_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_ibXEWZiiTZ"
   },
   "outputs": [],
   "source": [
    "my_data_char = torchtext.data.TabularDataset(\"cds.csv\", format=\"CSV\", \n",
    "                                        fields=[(\"sequence\", TEXT_CHAR)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mbO1nVm2iiMk"
   },
   "outputs": [],
   "source": [
    "train_char, test_char = my_data_char.split(split_ratio=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MVyncPjojbJE"
   },
   "outputs": [],
   "source": [
    "TEXT_CHAR.build_vocab(train_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "colab_type": "code",
    "id": "myhHYBhzjbED",
    "outputId": "87e3ad02-da18-4d10-e129-9c95ab8ae35a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function torchtext.vocab._default_unk_index>,\n",
       "            {'<pad>': 1,\n",
       "             '<start>': 6,\n",
       "             '<unk>': 0,\n",
       "             'a': 5,\n",
       "             'c': 3,\n",
       "             'g': 2,\n",
       "             't': 4})"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT_CHAR.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Z1A1bsLja8_"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    " \n",
    "#we don't split genes\n",
    "train_iter_char_bucket, test_iter_char_bucket = torchtext.data.BucketIterator.splits(\n",
    "    (train_char, test_char), batch_sizes=(10,10), sort_within_batch=False, sort_key=lambda x : len(x.sequence),\n",
    "    device=torch.device(device, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dpCF952mmObK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "44j9g4zwmOUP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zBScOiR3UPNR"
   },
   "source": [
    "## Make frequency table for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qi6H086PURgd"
   },
   "outputs": [],
   "source": [
    "def count_codons_iter(train_iter):\n",
    "  ''' Calculate the counts of each codon given subset of a CDS\n",
    "  \n",
    "  Args:\n",
    "    Train iter\n",
    "    \n",
    "  Returns:\n",
    "    dict{str, int}: dictionary with codons as key and corresponding number of occurences\n",
    "  \n",
    "  '''\n",
    "  codons_dict = CodonUsage.CodonsDict.copy()\n",
    "  for i, batch in enumerate(train_iter):\n",
    "\n",
    "    # Select for all non zero tensors\n",
    "    # Use this to find all indices that aren't padding\n",
    "    seq_len = batch.sequence.shape[\"seqlen\"]\n",
    "    batch_size = batch.sequence.shape[\"batch\"]\n",
    "    seq = batch.sequence.detach().cpu().numpy()\n",
    "    for batch_item in range(batch_size): \n",
    "      for seq_item in range(0, seq_len):\n",
    "        word = seq[seq_item, batch_item]\n",
    "        if TEXT.vocab.itos[word] in codon_to_aa: \n",
    "          codons_dict[TEXT.vocab.itos[word].upper()] += 1\n",
    "        elif TEXT.vocab.itos[word] == \"<pad>\": \n",
    "          # Break loop if we are at padding...\n",
    "          seq_item = seq_len + 1\n",
    "          \n",
    "   \n",
    "  return codons_dict\n",
    "\n",
    "def count_codons_fasta(fasta_file):\n",
    "  ''' Calculate the counts of each codon given a set of CDS\n",
    "  \n",
    "  Args:\n",
    "    Fasta file \n",
    "    \n",
    "  Returns:\n",
    "    dict{str, int}: dictionary with codons as key and corresponding number of occurences\n",
    "  \n",
    "  '''\n",
    "  \n",
    "  codons_dict = CodonUsage.CodonsDict.copy()\n",
    "  \n",
    "  for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "    seq = record.seq\n",
    "    if len(seq) % 3 != 0:\n",
    "      continue\n",
    "      \n",
    "    #count the codons for this sequence\n",
    "    for codon_start in range(0, len(seq), 3):\n",
    "      codons_dict[str(seq[codon_start:codon_start+3])] += 1\n",
    "   \n",
    "  return codons_dict\n",
    "\n",
    "def calculate_codon_frequency(codon_counts):\n",
    "  ''' Calculate the counts of each codon given a set of CDS\n",
    "  \n",
    "  Args:\n",
    "    codon usage table\n",
    "    \n",
    "  Returns:\n",
    "    dict{str, float}: dictionary with codons as key and corresponding \n",
    "    frequency of codon for AA\n",
    "  \n",
    "  '''\n",
    "  codon_freqs = CodonUsage.CodonsDict.copy()\n",
    "  \n",
    "  for _, synonymous_codons in CodonUsage.SynonymousCodons.items():\n",
    "    total_AA_count = sum([codon_counts[codon] for codon in synonymous_codons])\n",
    "    \n",
    "    if total_AA_count == 0:\n",
    "      continue\n",
    "      \n",
    "    for codon in synonymous_codons:\n",
    "      codon_freqs[codon] = codon_counts[codon] / total_AA_count\n",
    "  \n",
    "  return codon_freqs\n",
    "\n",
    "eColi_codon_counts = count_codons_fasta(\"ecoli_CDS.fasta\")\n",
    "# eColi_codon_counts = count_codons_iter(train_iter_bucket)\n",
    "eColi_codon_table = calculate_codon_frequency(eColi_codon_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZmOIRRRr0Io"
   },
   "outputs": [],
   "source": [
    "def make_unigram_conversion(train_iter): \n",
    "  ''' Help make table... '''\n",
    "  eColi_codon_counts = count_codons_iter(train_iter)\n",
    "  eColi_codon_table = calculate_codon_frequency(eColi_codon_counts)\n",
    "  unigram_freq_tbl = torch.tensor(np.array([[eColi_codon_table[codon_2.upper()] if (codon in codon_to_aa and codon_2 in codon_to_aa and codon_to_aa[codon] == codon_to_aa[codon_2]) else 0 \n",
    "             for codon_2 in TEXT.vocab.itos] \n",
    "            for index, codon in enumerate(TEXT.vocab.itos)])).to(device)\n",
    "  \n",
    "  pad_index = TEXT.vocab.stoi[\"<pad>\"]\n",
    "  unigram_freq_tbl[pad_index,pad_index] = 1\n",
    "  return unigram_freq_tbl \n",
    "\n",
    "\n",
    "unigram_freq_tbl = make_unigram_conversion(train_iter_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVb8Du80C0Z5"
   },
   "source": [
    "#### N gram frequency helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uMBilqW2wlyo"
   },
   "outputs": [],
   "source": [
    "def make_n_gram(train_iter, n, amino_acid_conversion):\n",
    "  '''\n",
    "  n : the number to each side...\n",
    "      e.g. 0 gram corresponds to freq, 1 gram corresponds to 3, etc.\n",
    "  amino_acid_conversion: tensor used to convert seq to AA one hots\n",
    "  '''\n",
    "  with torch.no_grad():\n",
    "    n_grams = []\n",
    "    targets = []\n",
    "    ident_mat = np.eye(len(TEXT.vocab.stoi))\n",
    "    ident_mat_aa = np.eye(len(AA_LABEL.vocab))\n",
    "    for i, batch in enumerate(train_iter):\n",
    "      # Select for all non zero tensors\n",
    "      # Use this to find all indices that aren't padding\n",
    "      seq_len = batch.sequence.shape[\"seqlen\"]\n",
    "      batch_size = batch.sequence.shape[\"batch\"]\n",
    "\n",
    "      # Pad amino acids and seq with <pad> token \n",
    "      pad_token = TEXT.vocab.stoi[\"<pad>\"]\n",
    "      additional_padding = ntorch.ones(batch_size, n, \n",
    "                                      names=(\"batch\", \"seqlen\")).long()\n",
    "      additional_padding *= pad_token\n",
    "      \n",
    "      seq = ntorch.cat([additional_padding, batch.sequence, additional_padding],\n",
    "                      dim=\"seqlen\")\n",
    "      \n",
    "      # Now one hots.. \n",
    "      amino_acids = amino_acid_conversion[seq.values].detach().cpu().numpy()\n",
    "      # Note: we should assert that start and pad are treated the same\n",
    "      # This is because at test time, presumably we narrow the start for the AA.. \n",
    "      if i == 0:\n",
    "        assert((amino_acids[0,n] == amino_acids[0,0]).all())\n",
    "      \n",
    "      seq = seq.detach().cpu().numpy()\n",
    "      # Pad with padding token\n",
    "      for batch_item in range(batch_size): \n",
    "        # start at n, end at seq_len - n\n",
    "        for seq_item in range(n, seq_len - n):\n",
    "          # Middle token is a discrete number representing the codon (0 to 66)\n",
    "          middle_token = seq[batch_item, seq_item]\n",
    "          # N gram is a 2d numpy array containing an amino acid embedding in each row\n",
    "          n_gram = amino_acids[batch_item,seq_item - n : seq_item + n + 1]\n",
    "          \n",
    "          # If we want all one hots:\n",
    "          # n_grams.append(ident_mat_aa[n_gram])\n",
    "          # targets.append(ident_mat[middle_token])\n",
    "\n",
    "          # If we want all indices:\n",
    "          n_grams.append(n_gram)\n",
    "          targets.append(middle_token)\n",
    "          \n",
    "  return n_grams, targets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_dictionary(n_grams, targets): \n",
    "  ''' Takes n grams and targets and makes them into '''\n",
    "  default_obj = lambda : torch.tensor(np.zeros(len(TEXT.vocab.stoi)))\n",
    "  default_dict = defaultdict(default_obj)\n",
    "  \n",
    "  for n_gram, target in zip(n_grams, targets): \n",
    "    default_dict[str(n_gram)][target] += 1\n",
    "    \n",
    "  for key in default_dict: \n",
    "    default_dict[key] /= (default_dict[key]).sum()\n",
    "    \n",
    "  return default_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "colab_type": "code",
    "id": "Vp0w6yso_kHz",
    "outputId": "80cc5d10-666a-4e89-900b-032330f5c7c8"
   },
   "outputs": [],
   "source": [
    "# n_grams are indices in amino acid space, targets are indices in codon space\n",
    "# n_grams, targets = make_n_gram(train_iter_bucket, 0, codon_to_aa_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RPMm5nyPWvvS"
   },
   "source": [
    "# Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fIVy7J0ceLkA"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "caIqk_EOmaDc"
   },
   "source": [
    "### Codon Level Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZZoA2XV9mivw"
   },
   "source": [
    "#### AA_COMPRESS\n",
    "A model to compress a codon sequence into its amino acid representation\n",
    "Can be easily used by passing in an embedding that turns each amino acid into a onehot OR each amino acid into a frequency table representation for its codon (unigram model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tl1h-vYXXQqE"
   },
   "outputs": [],
   "source": [
    "class AA_COMPRESS(ntorch.nn.Module):\n",
    "  '''\n",
    "  Compress info for codon sequence at the amino acid level\n",
    "  '''\n",
    "  \n",
    "  def __init__(self, params): \n",
    "    super(AA_COMPRESS, self).__init__()\n",
    "    self.codon_to_aa = params[\"CODON_TO_AA\"]\n",
    "    self.aa_embed = (ntorch.nn.Embedding.from_pretrained(self.codon_to_aa)\n",
    "                     .spec(\"seqlen\", \"hiddenlen\"))\n",
    "    \n",
    "    # don't learn these.. \n",
    "    self.aa_embed.weight.requires_grad_(False)  \n",
    "    \n",
    "  def forward(self, seq): \n",
    "    return self.aa_embed(seq)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4dPiQI0ddUaH"
   },
   "source": [
    "#### AA_NGRAM\n",
    "A model class that will take a string of codons and turn them into amino acids, then turn those amino acids into n gram based frequencies for what codon should be predicted in each position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nCYYQR2adxU7"
   },
   "outputs": [],
   "source": [
    "class AA_NGRAM(ntorch.nn.Module):\n",
    "  '''\n",
    "  Compress info for codon sequence at the amino acid level\n",
    "  TODO: Ignore pading predicts in forward pass to save time\n",
    "  '''\n",
    "  \n",
    "  def __init__(self, params): \n",
    "    super(AA_NGRAM, self).__init__()\n",
    "    self.codon_to_aa = params[\"CODON_TO_AA\"]\n",
    "    self.dict_list = params[\"N_GRAM_DICTS\"]\n",
    "    # How many indcies each dict takes\n",
    "    self.n_list = params[\"N_LIST\"]\n",
    "    # probability to apply to each n gram used\n",
    "    self.weight_list = params[\"WEIGHT_LIST\"]\n",
    "    self.longest_n = max(self.n_list)\n",
    "    self.out_vocab = params[\"OUT_VOCAB\"]\n",
    "    \n",
    "  def forward(self, seq): \n",
    "    \n",
    "    seq_len = seq.shape[\"seqlen\"]\n",
    "    batch_size = seq.shape[\"batch\"]\n",
    "      \n",
    "    pad_token = TEXT.vocab.stoi[\"<pad>\"]\n",
    "    additional_padding = ntorch.ones(batch_size, self.longest_n, \n",
    "                                    names=(\"batch\", \"seqlen\")).long()\n",
    "    additional_padding *= pad_token\n",
    "    \n",
    "    seq = ntorch.cat([additional_padding, seq, additional_padding],\n",
    "                    dim=\"seqlen\")\n",
    "    \n",
    "    \n",
    "    amino_acids = self.codon_to_aa[seq.values]\n",
    "    \n",
    "    return_ar = ntorch.zeros(seq_len, batch_size, self.out_vocab,\n",
    "                             names=(\"seqlen\", \"batch\", \"vocablen\"))\n",
    "    \n",
    "    # convert to numpy to leave GPU \n",
    "    amino_acids = amino_acids.detach().cpu().numpy()\n",
    "    for batch_item in range(batch_size): \n",
    "      # start at n, end at seq_len - n\n",
    "      for seq_item in range(self.longest_n, seq_len - self.longest_n):        \n",
    "        # Must iterate over all dictionaries\n",
    "        for weight, n, ngram_dict in zip(self.weight_list, \n",
    "                                         self.n_list, self.dict_list):          \n",
    "          # N gram is a 2d numpy array containing an amino acid embedding in each row\n",
    "          n_gram = amino_acids[batch_item,seq_item - n : seq_item + n + 1]\n",
    "\n",
    "          # note, we want to populate the return ar before padding!\n",
    "          return_ar[{\"seqlen\" : seq_item - self.longest_n, \n",
    "                     \"batch\" : batch_item}] += weight * ngram_dict[str(n_gram)].float()\n",
    "\n",
    "    return return_ar\n",
    "  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FGKLRU1-mrwj"
   },
   "source": [
    "#### AA_BILSTM\n",
    "A model to compress a codon sequence into its amino acid representation and then run a bidirectional LSTM over this sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ouCELjHjmxHw"
   },
   "outputs": [],
   "source": [
    "class AA_BILSTM(ntorch.nn.Module):\n",
    "  '''\n",
    "  Compress info for codon sequence at the amino acid level\n",
    "  '''\n",
    "  \n",
    "  def __init__(self, params): \n",
    "    super(AA_BILSTM, self).__init__()\n",
    "    self.codon_to_aa = params[\"CODON_TO_AA\"]\n",
    "    self.embedding_size = params[\"EMBED_DIM\"]\n",
    "    self.hiddenlen = params[\"HIDDEN_LEN\"]\n",
    "    self.num_layers = params[\"NUM_LAYERS\"]\n",
    "    self.lstm_dropout = params[\"LSTM_DROPOUT\"]\n",
    "    self.bidirectional = params[\"BIDIRECTIONAL\"]\n",
    "    self.num_directions = 1\n",
    "    if self.bidirectional:\n",
    "      self.num_directions = 2\n",
    "    \n",
    "    self.aa_embed = (ntorch.nn.Embedding.from_pretrained(self.codon_to_aa)\n",
    "                     .spec(\"seqlen\", \"embedlen\"))\n",
    "    \n",
    "    # don't learn these.. \n",
    "    self.aa_embed.weight.requires_grad_(False)  \n",
    "    \n",
    "    self.LSTM = (ntorch.nn.LSTM(self.embedding_size, self.hiddenlen,\n",
    "                                num_layers=self.num_layers, \n",
    "                                bidirectional=self.bidirectional\n",
    "                               )\n",
    "                .spec(\"embedlen\", \"seqlen\", name_out=\"hiddenlen\")\n",
    "                )\n",
    "    \n",
    "    \n",
    "  def forward(self, seq): \n",
    "    '''\n",
    "    Forward pass\n",
    "    ''' \n",
    "    aa_rep = self.aa_embed(seq)    \n",
    "    h_0 = ntorch.zeros(self.num_layers * self.num_directions, aa_rep.shape[\"batch\"], self.hiddenlen, \n",
    "                        names=(\"layers\", \"batch\", \"hiddenlen\")).to(device)\n",
    "    c_0 = ntorch.zeros(self.num_layers * self.num_directions, aa_rep.shape[\"batch\"], self.hiddenlen, \n",
    "                        names=(\"layers\", \"batch\", \"hiddenlen\")).to(device)\n",
    "    \n",
    "    h_0 = h_0.transpose(\"batch\", \"layers\", \"hiddenlen\")\n",
    "    c_0 = c_0.transpose(\"batch\", \"layers\", \"hiddenlen\")\n",
    "    hidden_states, (h_n, c_n) = self.LSTM(aa_rep, (h_0, c_0))\n",
    "    \n",
    "    return hidden_states\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JtfO85lwm5nO"
   },
   "source": [
    "#### Frequency Based Model\n",
    "A unigram model that only uses a single amino acid to do prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K-0CQkRjm4ss"
   },
   "outputs": [],
   "source": [
    "class FreqModel(ntorch.nn.Module):\n",
    "  ''' \n",
    "  Simple language model that uses the frequencies of the amino acids for modeling\n",
    "  Magic happens in aa_info model\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super(FreqModel, self).__init__()\n",
    "\n",
    "  def forward(self, text, aa_info):\n",
    "    ''' \n",
    "      Pass in context for the next amino acid\n",
    "    '''\n",
    "\n",
    "    return aa_info.rename(\"hiddenlen\", \"vocablen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i0159i6ynCf-"
   },
   "source": [
    "#### Codon Level Language Model \n",
    "A model that uses the previously predicted (or true) codons in addition to provided information about the amino acid for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dHmsrfdLmxB1"
   },
   "outputs": [],
   "source": [
    "class NNLM(ntorch.nn.Module):\n",
    "  ''' \n",
    "  Simple LSTM class.\n",
    "  '''\n",
    "  def __init__(self, params):\n",
    "    super(NNLM, self).__init__()\n",
    "    self.vocab_size = params[\"VOCAB_SIZE\"]\n",
    "    self.embedding_size = params[\"EMBED_DIM\"]\n",
    "    self.hiddenlen = params[\"HIDDEN_LEN\"]\n",
    "    self.num_layers = params[\"NUM_LAYERS\"]\n",
    "    self.linear_dropout = ntorch.nn.Dropout(p=params[\"LINEAR_DROPOUT\"])\n",
    "    self.lstm_dropout = params[\"LSTM_DROPOUT\"]\n",
    "    self.out_vocab = params[\"OUT_VOCAB\"]\n",
    "    self.aa_info_size = params[\"AA_COMPRESS_SIZE\"]\n",
    "    self.teacher_force_prob = params[\"TEACHER_FORCE\"]\n",
    "    if self.embedding_size is not None: \n",
    "      self.embedding = ntorch.nn.Embedding(num_embeddings=params[\"VOCAB_SIZE\"], \n",
    "                                           embedding_dim = self.embedding_size).spec(\"seqlen\", \"embedlen\")\n",
    "    else: \n",
    "      self.embedding = (ntorch.nn.Embedding.\n",
    "                        from_pretrained(torch.eye(len(TEXT.vocab.itos))\n",
    "                                       )\n",
    "                       ).spec(\"seqlen\", \"embedlen\")\n",
    "      \n",
    "      self.embedding.weight.requires_grad_(False)\n",
    "      self.embedding_size = len(TEXT.vocab.itos)\n",
    "    \n",
    "    self.LSTM = (ntorch.nn.LSTM(self.embedding_size, self.hiddenlen, num_layers=self.num_layers)\n",
    "                .spec(\"embedlen\", \"seqlen\", name_out=\"hiddenlen\")\n",
    "                )\n",
    "    self.linear = (ntorch.nn.Linear(self.hiddenlen + self.aa_info_size, \n",
    "                                    self.out_vocab)\n",
    "                   .spec(\"hiddenlen\", \"vocablen\")\n",
    "                  )\n",
    "    \n",
    "  def set_to_eval(self):\n",
    "    self.dropout.eval()\n",
    "  \n",
    "  def set_teacher_force(self, new_prob):\n",
    "    self.teacher_force_prob = new_prob\n",
    "    \n",
    "  def forward(self, text, aa_info):\n",
    "    ''' \n",
    "      Pass in context for the next amino acid\n",
    "    '''\n",
    "    \n",
    "    # Reset for each new batch...\n",
    "    h_0 = ntorch.zeros(text.shape[\"batch\"], self.num_layers, self.hiddenlen, \n",
    "                        names=(\"batch\", \"layers\", \"hiddenlen\")).to(device)\n",
    "    c_0 = ntorch.zeros(text.shape[\"batch\"], self.num_layers, self.hiddenlen, \n",
    "                        names=(\"batch\", \"layers\", \"hiddenlen\")).to(device)\n",
    " \n",
    "    # If we should use all the sequence as input\n",
    "    if self.teacher_force_prob == 1: \n",
    "      text_embedding = self.embedding(text)\n",
    "      hidden_states, (h_n, c_n) = self.LSTM(text_embedding, (h_0, c_0))\n",
    "      output = self.linear_dropout(hidden_states)\n",
    "      output = ntorch.cat([output, aa_info], dim=\"hiddenlen\")\n",
    "      output = self.linear(output)\n",
    "    \n",
    "    # If we should use some combination of teacher forcing\n",
    "    else: \n",
    "      # Use for teacher forcing...\n",
    "      outputs = []\n",
    "      model_input = text[{\"seqlen\" : slice(0, 1)}]\n",
    "      for position in range(text.shape[\"seqlen\"]): \n",
    "        text_embedding = self.embedding(model_input)\n",
    "        hidden_states, (h_n, c_n) = self.LSTM(text_embedding, (h_0, c_0))\n",
    "\n",
    "        output = self.linear_dropout(hidden_states)\n",
    "        aa_info_subset = aa_info[{\"seqlen\" : slice(position, position+1)}]\n",
    "        output = ntorch.cat([output, aa_info_subset], dim=\"hiddenlen\")\n",
    "\n",
    "        output = self.linear(output)\n",
    "        outputs.append(output)\n",
    "\n",
    "        # Define next input... \n",
    "        if random.random() < self.teacher_force_prob: \n",
    "          model_input = text[{\"seqlen\" : slice(position, position+1)}]\n",
    "        else: \n",
    "          # TODO: Should we be masking this output?\n",
    "          model_input = output.argmax(\"vocablen\")\n",
    "          \n",
    "      output = ntorch.cat(outputs, dim=\"seqlen\")\n",
    "    return output\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UDyAMW1nOIa"
   },
   "source": [
    "#### AA_ONLY \n",
    "A model that only uses a bidirectional LSTM over the amino acids in order to do prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kmba1P1CnNs6"
   },
   "outputs": [],
   "source": [
    "class AA_ONLY(ntorch.nn.Module):\n",
    "  ''' \n",
    "  Simple model to predict the output codons using only the input amino acid\n",
    "  '''\n",
    "  def __init__(self, params):\n",
    "    super(AA_ONLY, self).__init__()\n",
    "    self.out_vocab = params[\"OUT_VOCAB\"]\n",
    "    self.aa_info_size = params[\"AA_COMPRESS_SIZE\"]\n",
    "    self.linear = (ntorch.nn.Linear(self.aa_info_size, \n",
    "                                    self.out_vocab)\n",
    "                   .spec(\"hiddenlen\", \"vocablen\")\n",
    "                  )\n",
    "    \n",
    "  def forward(self, text, aa_info):\n",
    "    ''' \n",
    "      Pass in context for the next amino acid\n",
    "    '''\n",
    "\n",
    "    output = aa_info\n",
    "    output = self.linear(output)\n",
    "    return output\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ClG_oqYXmdpI"
   },
   "source": [
    "### Nucleotide Level Models\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sza5q9G2aadG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "noFpLqcOeN9X"
   },
   "source": [
    "## Train, Acc, PPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x--R1qhVcprx"
   },
   "outputs": [],
   "source": [
    "def train_model(train_iter, model, aa_compress, train_params, \n",
    "                optimizer, optimizer_aa=None):\n",
    "  '''\n",
    "  Train a given model \n",
    "  \n",
    "  TODO: \n",
    "  - Mask all non synonymous codons in test only\n",
    "    For now, we can do this at prediction time.. \n",
    "  - Precompute the text / target in the iterator function \n",
    "  \n",
    "  NOTE: \n",
    "    Optimizer_aa is optional if we want to learn on the compressions.\n",
    "  ''' \n",
    "  \n",
    "  model.train()\n",
    "  if \"TEACHER_FORCE\" in train_params:\n",
    "    model.teacher_force_prob = train_params[\"TEACHER_FORCE\"]\n",
    "\n",
    "  if optimizer_aa is not None:\n",
    "    # Does this accidentally turn on gradients? \n",
    "    aa_compress.train()\n",
    "    \n",
    "  loss_function = ntorch.nn.CrossEntropyLoss().spec(\"vocablen\")\n",
    "  model.to(device)\n",
    "  aa_compress.to(device)\n",
    "  loss_values = []\n",
    "  for epoch in range(train_params[\"num_epochs\"]):\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(train_iter):\n",
    "      model.zero_grad()\n",
    "      if optimizer_aa: \n",
    "        aa_compress.zero_grad()\n",
    "\n",
    "      \n",
    "      # Select for all non zero tensors\n",
    "      # Use this to find all indices that aren't padding\n",
    "      seq_len = batch.sequence.shape[\"seqlen\"]\n",
    "      text = batch.sequence.narrow(\"seqlen\", 0, seq_len - 1)\n",
    "      target = batch.sequence.narrow(\"seqlen\", 1, seq_len - 1)\n",
    "      \n",
    "      stacked_target = target.stack(dims=(\"batch\", \"seqlen\"), \n",
    "                                          name=\"seqlen\")\n",
    "\n",
    "      mask = (stacked_target != TEXT.vocab.stoi[\"<pad>\"])\n",
    "      prop_indices = (ntorch.nonzero(mask)\n",
    "                      .get(\"inputdims\", 0)\n",
    "                     )\n",
    "      # Forward\n",
    "      predictions = model(text, aa_compress(target)) \n",
    "      \n",
    "      # Stack the predictions into one long vector\n",
    "      predictions = predictions.stack(dims=(\"batch\", \"seqlen\"), name=\"seqlen\")\n",
    "            \n",
    "      # Only find loss on sequences\n",
    "      # TODO: Divide by batch size... \n",
    "      loss = loss_function(predictions.index_select(\"seqlen\", prop_indices),\n",
    "                           stacked_target.index_select(\"seqlen\", prop_indices))\n",
    "      \n",
    "      epoch_loss += loss.item()\n",
    "      \n",
    "      loss.backward()\n",
    "      # gradient clip\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), train_params[\"grad_clip\"])\n",
    "      optimizer.step()\n",
    "      \n",
    "      if optimizer_aa: \n",
    "        torch.nn.utils.clip_grad_norm_(aa_compress.parameters(), train_params[\"grad_clip\"])\n",
    "        optimizer_aa.step()\n",
    "\n",
    "      \n",
    "\n",
    "    print(\"Epoch: {} -- Loss: {}\".format(epoch, epoch_loss))        \n",
    "    loss_values.append(epoch_loss)\n",
    "\n",
    "  if train_params[\"plot_loss\"]: \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot([t for t in range(len(loss_values))], loss_values)\n",
    "    ax.set(xlabel='Epochs', ylabel='Loss', title='Loss during Optimization')\n",
    "    plt.show()\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHHHEu5CbHjF"
   },
   "source": [
    "#### Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1qKd_VyCJhIJ"
   },
   "outputs": [],
   "source": [
    "def accuracy(data_iter, model, aa_compress, teacher_force=0):\n",
    "  '''\n",
    "  Produce accuracy (# correct / total)\n",
    "  TODO: \n",
    "  - Mask all non synonymous codons in train\n",
    "    For now, we can do this at prediction time.. \n",
    "  - Precompute the text / target in the iterator function \n",
    "  ''' \n",
    "  \n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "  model.teacher_force_prob = teacher_force\n",
    "  aa_compress.to(device)\n",
    "  aa_compress.eval()\n",
    "  num_correct = 0 \n",
    "  num_total = 0 \n",
    "  with torch.no_grad():\n",
    "    for i, batch in enumerate(data_iter):\n",
    "      \n",
    "      # Select for all non zero tensors\n",
    "      # Use this to find all indices that aren't padding\n",
    "      seq_len = batch.sequence.shape[\"seqlen\"]\n",
    "      text = batch.sequence.narrow(\"seqlen\", 0, seq_len - 1)\n",
    "      target = batch.sequence.narrow(\"seqlen\", 1, seq_len - 1)\n",
    "      \n",
    "      stacked_target = target.stack(dims=(\"batch\", \"seqlen\"), \n",
    "                                          name=\"seqlen\")\n",
    "      mask = (stacked_target != TEXT.vocab.stoi[\"<pad>\"])\n",
    "      prop_indices = (ntorch.nonzero(mask)\n",
    "                      .get(\"inputdims\", 0)\n",
    "                     ).rename(\"elements\", \"seqlen\")\n",
    "      # Forward\n",
    "      predictions = model(text, aa_compress(target)) \n",
    "      # Mask all outputs that don't work\n",
    "      mask_bad_codons = ntorch.tensor(mask_tbl[target.values], \n",
    "                         names=(\"seqlen\", \"batch\", \"vocablen\")).float()\n",
    "      predictions = (mask_bad_codons.double() + predictions.double())\n",
    "\n",
    "     \n",
    "      # Stack the predictions into one long vector\n",
    "      predictions = predictions.stack(dims=(\"batch\", \"seqlen\"), name=\"seqlen\")\n",
    "\n",
    "      predictions = predictions.argmax(\"vocablen\").index_select(\"seqlen\", \n",
    "                                                                prop_indices)\n",
    "      stacked_target = stacked_target.index_select(\"seqlen\", prop_indices)\n",
    "            \n",
    "      num_correct += (predictions == stacked_target).sum().item()   \n",
    "      num_total += predictions.shape[\"seqlen\"]\n",
    "      \n",
    "      # For quick results, toggle this\n",
    "#       if i == 20: \n",
    "#         break\n",
    "      \n",
    "#   return num_correct / num_total \n",
    "\n",
    "  return (predictions, stacked_target, num_correct / num_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZqmKvOqbDA3"
   },
   "source": [
    "#### PPL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BeX8qB2RYSyY"
   },
   "outputs": [],
   "source": [
    "def PPL(data_iter, model, aa_compress, teacher_force=0):\n",
    "  '''\n",
    "  Produce average ppl of prediction\n",
    "  TODO: \n",
    "  - Mask all non synonymous codons in train\n",
    "    For now, we can do this at prediction time.. \n",
    "  - Precompute the text / target in the iterator function \n",
    "  ''' \n",
    "  \n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "  model.teacher_force_prob = teacher_force\n",
    "  aa_compress.to(device)\n",
    "  aa_compress.eval()\n",
    "  ppl = 0\n",
    "  num_total = 0 \n",
    "  loss_function = ntorch.nn.CrossEntropyLoss(reduction=\"none\").spec(\"vocablen\")\n",
    "  with torch.no_grad():\n",
    "    for i, batch in enumerate(data_iter):\n",
    "      \n",
    "      # Select for all non zero tensors\n",
    "      # Use this to find all indices that aren't padding\n",
    "      seq_len = batch.sequence.shape[\"seqlen\"]\n",
    "      text = batch.sequence.narrow(\"seqlen\", 0, seq_len - 1)\n",
    "      target = batch.sequence.narrow(\"seqlen\", 1, seq_len - 1)\n",
    "      \n",
    "      stacked_target = target.stack(dims=(\"batch\", \"seqlen\"), \n",
    "                                          name=\"seqlen\")\n",
    "      mask = (stacked_target != TEXT.vocab.stoi[\"<pad>\"])\n",
    "      prop_indices = (ntorch.nonzero(mask)\n",
    "                      .get(\"inputdims\", 0)\n",
    "                     ).rename(\"elements\", \"seqlen\")\n",
    "      # Forward\n",
    "      predictions = model(text, aa_compress(target)) \n",
    "      # Mask all outputs that don't work\n",
    "      mask_bad_codons = ntorch.tensor(mask_tbl[target.values], \n",
    "                         names=(\"seqlen\", \"batch\", \"vocablen\")).float()\n",
    "      predictions = (mask_bad_codons.double() + predictions.double())\n",
    "\n",
    "     \n",
    "      # Stack the predictions into one long vector\n",
    "      predictions = predictions.stack(dims=(\"batch\", \"seqlen\"), name=\"seqlen\")\n",
    "      loss = loss_function(predictions.index_select(\"seqlen\", prop_indices),\n",
    "                     stacked_target.index_select(\"seqlen\", prop_indices))\n",
    "      ppl += loss.exp().sum().item()\n",
    "      num_total += loss.shape[\"seqlen\"]      \n",
    "      # For quick results, toggle this\n",
    "#       if i == 20: \n",
    "#         break\n",
    "\n",
    "  return ppl / num_total "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8-ZoDTVH8Kf"
   },
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yNaqhFpMH6oM"
   },
   "outputs": [],
   "source": [
    "def output_predictions_and_target(data_iter, model, aa_compress, teacher_force=0):\n",
    "  '''\n",
    "  Outputs matrix for prediction, target, and also the accuracy\n",
    "  ''' \n",
    "  \n",
    "  \n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "  model.teacher_force_prob = teacher_force\n",
    "  aa_compress.to(device)\n",
    "  aa_compress.eval()\n",
    "  num_correct = 0 \n",
    "  num_total = 0 \n",
    "  \n",
    "  prediction_list = []\n",
    "  target_list = []\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    for i, batch in enumerate(data_iter):\n",
    "      \n",
    "      # Select for all non zero tensors\n",
    "      # Use this to find all indices that aren't padding\n",
    "      seq_len = batch.sequence.shape[\"seqlen\"]\n",
    "      text = batch.sequence.narrow(\"seqlen\", 0, seq_len - 1)\n",
    "      target = batch.sequence.narrow(\"seqlen\", 1, seq_len - 1)\n",
    "      \n",
    "      stacked_target = target.stack(dims=(\"batch\", \"seqlen\"), \n",
    "                                          name=\"seqlen\")\n",
    "      mask = (stacked_target != TEXT.vocab.stoi[\"<pad>\"])\n",
    "      prop_indices = (ntorch.nonzero(mask)\n",
    "                      .get(\"inputdims\", 0)\n",
    "                     ).rename(\"elements\", \"seqlen\")\n",
    "      # Forward\n",
    "      predictions = model(text, aa_compress(target)) \n",
    "      # Mask all outputs that don't work\n",
    "      mask_bad_codons = ntorch.tensor(mask_tbl[target.values], \n",
    "                         names=(\"seqlen\", \"batch\", \"vocablen\")).float()\n",
    "      predictions = (mask_bad_codons.double() + predictions.double())\n",
    "      \n",
    "     \n",
    "      # Stack the predictions into one long vector\n",
    "      predictions = predictions.stack(dims=(\"batch\", \"seqlen\"), name=\"seqlen\")\n",
    "\n",
    "      predictions = predictions.argmax(\"vocablen\").index_select(\"seqlen\", \n",
    "                                                                prop_indices)\n",
    "      \n",
    "      prediction_list.append(predictions.cpu())\n",
    "      \n",
    "      stacked_target = stacked_target.index_select(\"seqlen\", prop_indices)\n",
    "      \n",
    "      target_list.append(stacked_target.cpu())\n",
    "            \n",
    "      num_correct += (predictions == stacked_target).sum().item()   \n",
    "      num_total += predictions.shape[\"seqlen\"]\n",
    "\n",
    "  return (prediction_list, target_list, num_correct / num_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E6fn1LDOYGXe"
   },
   "outputs": [],
   "source": [
    "def mismatched_indices(predict_list, target_list):\n",
    "  '''\n",
    "  Inputs prediction and true target, outputs mismatch list and mismatch positions\n",
    "  \n",
    "  mismatch list is a NUM_SEQUENCE x LEN_SEQUENCE list of lists, boolean values. 1 if prediction is correct, 0 if incorrect.\n",
    "  (e.g. [0, 1, 1, 0, 0, ...])\n",
    "  \n",
    "  msimatch positions contains positions in which misclassification happened (e.g. [3, 4, 9, 13, 31])\n",
    "  \n",
    "  '''\n",
    "  mismatch_list = []\n",
    "  for i in range(len(predict_list)):\n",
    "    mismatches = predict_list[i] != target_list[i]\n",
    "    mismatch_list.append(np.array(mismatches.values))\n",
    "  \n",
    "  mismatch_positions = []\n",
    "  for i in range(len(mismatch_list)):\n",
    "    positions = [j for j, x in enumerate(mismatch_list[i]) if x == 1]\n",
    "    mismatch_positions.append(positions)\n",
    "    \n",
    "  return (mismatch_list, mismatch_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xai-jkALZLfh"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import zip_longest\n",
    "\n",
    "def pool_mismatches(mismatch_list, mismatch_indices):\n",
    "  '''\n",
    "  Given mismatch list and mismatch positions, outputs error rate over sequence length\n",
    "  \n",
    "  METHOD: counts error in every position over all the test sequences. Misclassification at each position is counted,\n",
    "  and normalized over how many sequences have a position there. (e.g. all 132 sequences have a nt in position 1, \n",
    "  but only a few would have it at position 9432.)\n",
    "  \n",
    "  \n",
    "  '''\n",
    "  collapsed_array = []\n",
    "  for item in mismatch_indices:\n",
    "    collapsed_array.extend(item)\n",
    "    \n",
    "  \n",
    "  counts = Counter(collapsed_array)\n",
    "  \n",
    "  #normalize the counts\n",
    "  seq_lengths = []\n",
    "  for item in mismatch_list:\n",
    "    seq_lengths.append(len(item))\n",
    "      \n",
    "  normalizing_list = np.array([0] * max(seq_lengths))\n",
    "\n",
    "  for length in seq_lengths:\n",
    "    normalizing_list = [sum(n) for n in zip_longest(normalizing_list, [1]*length, fillvalue=0)]\n",
    "    \n",
    "  for i in range(max(seq_lengths)):\n",
    "    counts[i] = counts[i] / normalizing_list[i]\n",
    "    \n",
    "  normalized_counts = [0]*max(seq_lengths)\n",
    "  \n",
    "  for key in counts:\n",
    "    try:\n",
    "      normalized_counts[key] = counts[key]\n",
    "    except:\n",
    "      print (key)\n",
    "    \n",
    "  return normalized_counts\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "K2ou_qIWudZf",
    "outputId": "6649a774-aafa-4a47-e84c-c3092a694816"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-dadd4221f2bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmismatched_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_unigram_predicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_target_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmismatch_over_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool_mismatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_unigram_predicts' is not defined"
     ]
    }
   ],
   "source": [
    "out = mismatched_indices(test_unigram_predicts, test_target_vals)\n",
    "mismatch_over_sequence = pool_mismatches(out[0], out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "JI1Y5Aw9zNsz",
    "outputId": "8882b058-467a-466d-c411-cfc1bd9f01de"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-6734d37e0c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmismatch_over_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool_mismatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "mismatch_over_sequence = pool_mismatches(out[0], out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "Dx_B89CGyFvK",
    "outputId": "3e8e1350-0f80-46de-9d02-e8d804cf2f30"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJIVQuBNCSAr5/82Aw7IuIMMVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(len(mismatch_over_sequence))[:1000], mismatch_over_sequence[:1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "raAT8rR9ywy6",
    "outputId": "7794e16b-6733-49e9-f36a-d90fbf388a88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 132)"
      ]
     },
     "execution_count": 226,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w9dQq5ZIXbBs",
    "outputId": "5f42cc67-a7b3-467c-cb7c-dd6259c07949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n"
     ]
    }
   ],
   "source": [
    "print (\"HELLO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8rtOdj3dbEvX"
   },
   "source": [
    "####  Preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fwtUEnoFQRb"
   },
   "outputs": [],
   "source": [
    "def get_prediction(batch, model, aa_compress): \n",
    "  ''' Predict outputs from sequence'''\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    seq_len = batch.sequence.shape[\"seqlen\"]\n",
    "    text = batch.sequence.narrow(\"seqlen\", 0, seq_len - 1)\n",
    "    target = batch.sequence.narrow(\"seqlen\", 1, seq_len - 1)\n",
    "    # Forward\n",
    "    predictions = model(text, aa_compress(target)) \n",
    "    mask_bad_codons = ntorch.tensor(mask_tbl[target.values], \n",
    "                       names=(\"seqlen\", \"batch\", \"vocablen\")).float()\n",
    "    predictions = (mask_bad_codons + predictions)\n",
    "    predictions = predictions.argmax(\"vocablen\")\n",
    "  return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JN8cVZYMxkNw"
   },
   "source": [
    "## Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h4N3M6r1xwTV"
   },
   "outputs": [],
   "source": [
    "unigram_freq_tbl = make_unigram_conversion(train_iter_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "gTwN6sUwxmpg",
    "outputId": "d2dc7062-2af1-4aee-f937-9a09ccfad74f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FreqModel(), AA_COMPRESS(\n",
       "   (aa_embed): Embedding(67, 67)\n",
       " ))"
      ]
     },
     "execution_count": 124,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "aa_compress_params = {\n",
    "    \"CODON_TO_AA\" : unigram_freq_tbl\n",
    "}\n",
    "\n",
    "\n",
    "model = FreqModel()\n",
    "aa_compress = AA_COMPRESS(aa_compress_params)\n",
    "model.to(device), aa_compress.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yaBq11QXxmmh",
    "outputId": "5580f670-8a3e-43c1-d12a-373765486aba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.5195127808520912\n"
     ]
    }
   ],
   "source": [
    "test_unigram_predicts, test_target_vals, unigram_test_ac = output_predictions_and_target(test_iter_bucket, model, aa_compress)\n",
    "# train_unigram_predicts, train_target_vals, unigram_train_ac = prediction_mismatches(test_iter_bucket, model, aa_compress)\n",
    "\n",
    "# unigram_test_ac = accuracy(test_iter_bucket, model, aa_compress)\n",
    "\n",
    "# print(\"Train accuracy: \", unigram_train_ac)\n",
    "print(\"Test accuracy: \", unigram_test_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "i-5DE_LZXQoF",
    "outputId": "7a587a5b-d6c0-497a-b2a7-0ad5d3a91687"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedTensor(\n",
       "\ttensor([10, 19, 16,  ...,  4, 18, 62], device='cuda:0'),\n",
       "\t('seqlen',))"
      ]
     },
     "execution_count": 101,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_unigram_predicts[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VUglocblXVT8"
   },
   "outputs": [],
   "source": [
    "out = mismatched_indices(test_unigram_predicts, test_target_vals)\n",
    "mismatch_over_sequence = pool_mismatches(out[0], out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eG9Z2Yp92888"
   },
   "outputs": [],
   "source": [
    "out = mismatched_indices(test_unigram_predicts, test_target_vals)\n",
    "mismatch_over_sequence = pool_mismatches(out[0], out[1])\n",
    "plt.scatter(range(len(mismatch_over_sequence))[:300], mismatch_over_sequence[:300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "nzOavnI1xmi0",
    "outputId": "59c5a368-4f8e-43d9-edad-9f85992fdf92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train PPL:  3.379582556636873\n",
      "Test PPL:  3.395484078717863\n"
     ]
    }
   ],
   "source": [
    "test_ppl, train_ppl = (PPL(test_iter_bucket, model, aa_compress), \n",
    "                     PPL(train_iter_bucket, model, aa_compress))\n",
    "print(\"Train PPL: \", train_ppl)\n",
    "print(\"Test PPL: \", test_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "iu91yG0sxmez",
    "outputId": "72b0a4c2-8bfe-485f-b6b4-594f2f480254"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedTensor(\n",
       "\ttensor([10, 28,  8,  ..., 19,  2, 63]),\n",
       "\t('seqlen',))"
      ]
     },
     "execution_count": 97,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_unigram_predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yf1SN58HXNEn"
   },
   "source": [
    "## N-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BnrnYD05XL-d"
   },
   "outputs": [],
   "source": [
    "# n_grams are indices in amino acid space, targets are indices in codon space\n",
    "n_grams, targets = make_n_gram(train_iter_bucket, 0, codon_to_aa_index)\n",
    "zero_dict = build_dictionary(n_grams, targets)\n",
    "\n",
    "n_grams, targets = make_n_gram(train_iter_bucket, 1, codon_to_aa_index)\n",
    "one_dict = build_dictionary(n_grams, targets)\n",
    "\n",
    "n_grams, targets = make_n_gram(train_iter_bucket, 2, codon_to_aa_index)\n",
    "two_dict = build_dictionary(n_grams, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "lfrDwN2yXhuU",
    "outputId": "d3e971ab-3129-4fa8-8d8f-2aa83e155662"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FreqModel(), AA_NGRAM())"
      ]
     },
     "execution_count": 165,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa_params = {\n",
    "    # convert each index to the aa index\n",
    "    \"CODON_TO_AA\" : codon_to_aa_index,\n",
    "    \"N_GRAM_DICTS\" : [zero_dict, one_dict],\n",
    "    \"N_LIST\" : [0, 1],\n",
    "    \"WEIGHT_LIST\" : [0.5, 0.5],\n",
    "    \"OUT_VOCAB\" : len(TEXT.vocab.stoi)   \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = FreqModel()\n",
    "aa_compress = AA_NGRAM(aa_params)\n",
    "model.to(device), aa_compress.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "b8w3cVLIXhnR",
    "outputId": "0e2d6fd9-c8d1-4f3f-9ee4-e7e2d0491422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.5508248362762165\n",
      "Test accuracy:  0.48140406958873794\n"
     ]
    }
   ],
   "source": [
    "# train ac should be higher.. something probably wrong.\n",
    "test_ac, train_ac = (accuracy(test_iter_bucket, model, aa_compress),\n",
    "                     accuracy(train_iter_bucket, model, aa_compress))\n",
    "print(\"Train accuracy: \", train_ac)\n",
    "print(\"Test accuracy: \", test_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "I5buwbyHUZmw",
    "outputId": "61b34400-09c6-4354-8f21-e0cfa4588e04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FreqModel(), AA_NGRAM())"
      ]
     },
     "execution_count": 171,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa_params = {\n",
    "    # convert each index to the aa index\n",
    "    \"CODON_TO_AA\" : codon_to_aa_index,\n",
    "    \"N_GRAM_DICTS\" : [two_dict],# [zero_dict, one_dict],\n",
    "    \"N_LIST\" : [2],\n",
    "    \"WEIGHT_LIST\" : [1],\n",
    "    \"OUT_VOCAB\" : len(TEXT.vocab.stoi)   \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = FreqModel()\n",
    "aa_compress = AA_NGRAM(aa_params)\n",
    "model.to(device), aa_compress.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kw_D5fYBUZgU"
   },
   "outputs": [],
   "source": [
    "# train ac should be higher.. something probably wrong.\n",
    "test_ac, train_ac = (accuracy(test_iter_bucket, model, aa_compress),\n",
    "                     accuracy(train_iter_bucket, model, aa_compress))\n",
    "print(\"Train accuracy: \", train_ac)\n",
    "print(\"Test accuracy: \", test_ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "umHh-qplxi6N"
   },
   "source": [
    "## Language Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w511Nsl6xiZw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "4FP9Yq5ic7HV",
    "outputId": "2f4119be-e1a3-437c-f974-502ed8aaf653"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NNLM(\n",
       "   (linear_dropout): Dropout(p=0.2)\n",
       "   (embedding): Embedding(67, 67)\n",
       "   (LSTM): LSTM(67, 200, num_layers=3, batch_first=True)\n",
       "   (linear): Linear(in_features=223, out_features=67, bias=True)\n",
       " ), AA_COMPRESS(\n",
       "   (aa_embed): Embedding(67, 23)\n",
       " ))"
      ]
     },
     "execution_count": 132,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params = {\n",
    "    \"VOCAB_SIZE\" : len(TEXT.vocab),\n",
    "    \"EMBED_DIM\" : None, #50,\n",
    "    \"OUT_VOCAB\": len(TEXT.vocab),\n",
    "    \"HIDDEN_LEN\" : 200,\n",
    "    \"NUM_LAYERS\" : 3,\n",
    "    \"LINEAR_DROPOUT\" : 0.2,\n",
    "    \"LSTM_DROPOUT\" : 0.2,    \n",
    "    \"AA_COMPRESS_SIZE\" : index_table.shape[1],\n",
    "    \"TEACHER_FORCE\" : 1\n",
    "}\n",
    "\n",
    "aa_compress_params = {\n",
    "    \"CODON_TO_AA\" : index_table\n",
    "}\n",
    "\n",
    "\n",
    "train_params = {\n",
    "    \"num_epochs\":30, \n",
    "    \"lr\":1e-3,  \n",
    "    \"weight_decay\":1e-6,\n",
    "    \"device\":device, \n",
    "    \"grad_clip\": 10, \n",
    "    \"plot_loss\" : True,\n",
    "    \"TEACHER_FORCE\" : 1\n",
    "}\n",
    "\n",
    "\n",
    "model = NNLM(model_params)\n",
    "aa_compress = AA_COMPRESS(aa_compress_params)\n",
    "model.to(device), aa_compress.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 907
    },
    "colab_type": "code",
    "id": "D9PqYDIoem7K",
    "outputId": "15fe8904-2337-4764-f839-0606d5f1ed8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 -- Loss: 1131.959959745407\n",
      "Epoch: 1 -- Loss: 956.8145925998688\n",
      "Epoch: 2 -- Loss: 804.2731750011444\n",
      "Epoch: 3 -- Loss: 680.7200589179993\n",
      "Epoch: 4 -- Loss: 581.9270281791687\n",
      "Epoch: 5 -- Loss: 507.74945878982544\n",
      "Epoch: 6 -- Loss: 454.8769830465317\n",
      "Epoch: 7 -- Loss: 417.3328025341034\n",
      "Epoch: 8 -- Loss: 389.7979357242584\n",
      "Epoch: 9 -- Loss: 370.1917984485626\n",
      "Epoch: 10 -- Loss: 355.9717655181885\n",
      "Epoch: 11 -- Loss: 344.83047610521317\n",
      "Epoch: 12 -- Loss: 336.1347362399101\n",
      "Epoch: 13 -- Loss: 329.4694827198982\n",
      "Epoch: 14 -- Loss: 323.8495211005211\n",
      "Epoch: 15 -- Loss: 319.3141167163849\n",
      "Epoch: 16 -- Loss: 315.29802906513214\n",
      "Epoch: 17 -- Loss: 311.65382105112076\n",
      "Epoch: 18 -- Loss: 307.7892631292343\n",
      "Epoch: 19 -- Loss: 304.555812060833\n",
      "Epoch: 20 -- Loss: 301.6094952225685\n",
      "Epoch: 21 -- Loss: 297.9011378288269\n",
      "Epoch: 22 -- Loss: 294.08951783180237\n",
      "Epoch: 23 -- Loss: 290.21923410892487\n",
      "Epoch: 24 -- Loss: 287.0164201259613\n",
      "Epoch: 25 -- Loss: 283.3129064440727\n",
      "Epoch: 26 -- Loss: 279.5372940301895\n",
      "Epoch: 27 -- Loss: 275.4896043539047\n",
      "Epoch: 28 -- Loss: 272.1893350481987\n",
      "Epoch: 29 -- Loss: 268.1464375257492\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "NNLM(\n",
       "  (linear_dropout): Dropout(p=0.2)\n",
       "  (embedding): Embedding(67, 67)\n",
       "  (LSTM): LSTM(67, 200, num_layers=3, batch_first=True)\n",
       "  (linear): Linear(in_features=223, out_features=67, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 133,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_params[\"lr\"], weight_decay=train_params[\"weight_decay\"])\n",
    "train_model(train_iter_bucket, model, aa_compress, train_params, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "8LazIWmRgSaw",
    "outputId": "007f674c-4f9b-40f0-d301-878dec4168fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  (NamedTensor(\n",
      "\ttensor([10,  3, 36,  ...,  3, 57, 63], device='cuda:0'),\n",
      "\t('seqlen',)), NamedTensor(\n",
      "\ttensor([10,  3, 43,  ..., 24, 54, 65], device='cuda:0'),\n",
      "\t('seqlen',)), 0.4573393145671263)\n",
      "Test accuracy:  (NamedTensor(\n",
      "\ttensor([10,  7, 16,  ..., 18, 15, 63], device='cuda:0'),\n",
      "\t('seqlen',)), NamedTensor(\n",
      "\ttensor([10,  7, 44,  ..., 62, 15, 63], device='cuda:0'),\n",
      "\t('seqlen',)), 0.45734250204643406)\n"
     ]
    }
   ],
   "source": [
    "test_ac, train_ac = (accuracy(test_iter_bucket, model, aa_compress, teacher_force=0), \n",
    "                     accuracy(train_iter_bucket, model, aa_compress, teacher_force=0))\n",
    "print(\"Train accuracy: \", train_ac)\n",
    "print(\"Test accuracy: \", test_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9fSqJ8Sa6qf"
   },
   "outputs": [],
   "source": [
    "test_ppl, train_ppl = (PPL(test_iter_bucket, model, aa_compress, teacher_force=0), \n",
    "                     PPL(train_iter_bucket, model, aa_compress, teacher_force=0))\n",
    "print(\"Train PPL: \", train_ppl)\n",
    "print(\"Test PPL: \", test_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "colab_type": "code",
    "id": "KKbLsivZKEj9",
    "outputId": "213aaaed-4e27-49fc-a61a-6a02ef1e77dd"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-135-746554c4f2e2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    test_ac = (accuracy(test_iter_bucket, model, aa_compress, teacher_force=0)\u001b[0m\n\u001b[0m                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "test_ac = (accuracy(test_iter_bucket, model, aa_compress, teacher_force=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxQUBDakKHqY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0qiLM8P-FZ6"
   },
   "source": [
    "## BiLSTM AA Level Only\n",
    "\n",
    "Use only the Amino acids, not the true or predicted codons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rU5U5zDk-JlU"
   },
   "outputs": [],
   "source": [
    "\n",
    "aa_compress_params = {\n",
    "    \"CODON_TO_AA\" : index_table,\n",
    "    \"EMBED_DIM\" : index_table.shape[1],\n",
    "    \"HIDDEN_LEN\" : 50, \n",
    "    \"NUM_LAYERS\" : 3, \n",
    "    \"LSTM_DROPOUT\" : 0.1,\n",
    "    \"BIDIRECTIONAL\" : True\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "model_params = {\n",
    "    \"OUT_VOCAB\": len(TEXT.vocab),\n",
    "    \"AA_COMPRESS_SIZE\" : (aa_compress_params[\"HIDDEN_LEN\"] * \n",
    "                          (2 if aa_compress_params[\"BIDIRECTIONAL\"] else 1))\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "train_params = {\n",
    "    \"num_epochs\":20, \n",
    "    \"lr\":1e-3,  \n",
    "    \"weight_decay\":1e-6,\n",
    "    \"device\":device, \n",
    "    \"grad_clip\": 10, \n",
    "    \"plot_loss\" : True\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = AA_ONLY(model_params)\n",
    "aa_compress = AA_BILSTM(aa_compress_params)\n",
    "model.to(device), aa_compress.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9F_dTGAq-lG4"
   },
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_params[\"lr\"],\n",
    "                             weight_decay=train_params[\"weight_decay\"])\n",
    "optimizer_aa = torch.optim.Adam(aa_compress.parameters(), \n",
    "                                lr=train_params[\"lr\"], \n",
    "                                weight_decay=train_params[\"weight_decay\"])\n",
    "\n",
    "\n",
    "train_model(train_iter_bucket, model, aa_compress, train_params,\n",
    "            optimizer, optimizer_aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WRyZhZK-lBy"
   },
   "outputs": [],
   "source": [
    "test_ac, train_ac = (accuracy(test_iter_bucket, model, aa_compress), \n",
    "                     accuracy(train_iter_bucket, model, aa_compress))\n",
    "print(\"Train accuracy: \", train_ac)\n",
    "print(\"Test accuracy: \", test_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JZf_-wXI-Jch"
   },
   "outputs": [],
   "source": [
    "test_ppl, train_ppl = (PPL(test_iter_bucket, model, aa_compress), \n",
    "                       PPL(train_iter_bucket, model, aa_compress))\n",
    "print(\"Train PPL: \", train_ppl)\n",
    "print(\"Test PPL: \", test_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WLFSDti8_E3J"
   },
   "outputs": [],
   "source": [
    "print (AA_BILSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dD9FDxHXO0Kg"
   },
   "source": [
    "## RNN + BiLSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "g04As1G7O2k-",
    "outputId": "785354cd-7d5d-4d19-8b75-0b8300b56f1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NNLM(\n",
       "   (linear_dropout): Dropout(p=0.2)\n",
       "   (embedding): Embedding(67, 67)\n",
       "   (LSTM): LSTM(67, 100, num_layers=2, batch_first=True)\n",
       "   (linear): Linear(in_features=200, out_features=67, bias=True)\n",
       " ), AA_BILSTM(\n",
       "   (aa_embed): Embedding(67, 23)\n",
       "   (LSTM): LSTM(23, 50, num_layers=3, batch_first=True, bidirectional=True)\n",
       " ))"
      ]
     },
     "execution_count": 104,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "aa_compress_params = {\n",
    "    \"CODON_TO_AA\" : index_table,\n",
    "    \"EMBED_DIM\" : index_table.shape[1],\n",
    "    \"HIDDEN_LEN\" : 50, \n",
    "    \"NUM_LAYERS\" : 3, \n",
    "    \"LSTM_DROPOUT\" : 0.1,\n",
    "    \"BIDIRECTIONAL\" : True\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "model_params = {\n",
    "    \"VOCAB_SIZE\" : len(TEXT.vocab),\n",
    "    \"EMBED_DIM\" : None, #50,\n",
    "    \"OUT_VOCAB\": len(TEXT.vocab),\n",
    "    \"HIDDEN_LEN\" : 100,\n",
    "    \"NUM_LAYERS\" : 2,\n",
    "    \"LINEAR_DROPOUT\" : 0.2,\n",
    "    \"LSTM_DROPOUT\" : 0.2,    \n",
    "    \"AA_COMPRESS_SIZE\" : (aa_compress_params[\"HIDDEN_LEN\"] * \n",
    "                          (2 if aa_compress_params[\"BIDIRECTIONAL\"] else 1)),\n",
    "    \"TEACHER_FORCE\" : 1\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "train_params = {\n",
    "    \"num_epochs\":30, \n",
    "    \"lr\":1e-3,  \n",
    "    \"weight_decay\":1e-6,\n",
    "    \"device\":device, \n",
    "    \"grad_clip\": 20, \n",
    "    \"plot_loss\" : True,\n",
    "    \"teacher_force\" : 1\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = NNLM(model_params)\n",
    "aa_compress = AA_BILSTM(aa_compress_params)\n",
    "model.to(device), aa_compress.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "colab_type": "code",
    "id": "7nwzRfSVO2Sl",
    "outputId": "d7381190-ffcc-4da0-b135-53a8fbec52ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 -- Loss: 726.3023167848587\n",
      "Epoch: 1 -- Loss: 308.15690463781357\n",
      "Epoch: 2 -- Loss: 299.08718305826187\n",
      "Epoch: 3 -- Loss: 296.0259350538254\n",
      "Epoch: 4 -- Loss: 293.42268323898315\n",
      "Epoch: 5 -- Loss: 292.2179016470909\n",
      "Epoch: 6 -- Loss: 291.55719166994095\n",
      "Epoch: 7 -- Loss: 290.16761857271194\n",
      "Epoch: 8 -- Loss: 289.27620780467987\n",
      "Epoch: 9 -- Loss: 288.74894523620605\n",
      "Epoch: 10 -- Loss: 287.961663544178\n",
      "Epoch: 11 -- Loss: 287.9280743598938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-e9d6be3d7375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m train_model(train_iter_bucket, model, aa_compress, train_params,\n\u001b[0;32m----> 8\u001b[0;31m             optimizer, optimizer_aa)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-fd58cdcdd8db>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_iter, model, aa_compress, train_params, optimizer, optimizer_aa)\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m       \u001b[0;31m# gradient clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"grad_clip\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=train_params[\"lr\"],\n",
    "                             weight_decay=train_params[\"weight_decay\"])\n",
    "optimizer_aa = torch.optim.Adam(aa_compress.parameters(), \n",
    "                                lr=train_params[\"lr\"], \n",
    "                                weight_decay=train_params[\"weight_decay\"])\n",
    "\n",
    "train_model(train_iter_bucket, model, aa_compress, train_params,\n",
    "            optimizer, optimizer_aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "colab_type": "code",
    "id": "EXPKdzxyZdIn",
    "outputId": "ee9acd42-6fe1-468d-a15e-0e3858d510ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  (NamedTensor(\n",
      "\ttensor([10, 10, 11, 16, 51, 17, 18, 36, 36, 17, 17, 19, 48, 23, 17, 36, 50, 16,\n",
      "        36, 48, 37, 23,  4, 51, 12, 23, 63,  8, 27, 19,  9, 18, 53, 52, 22, 15,\n",
      "        51,  9, 63,  4, 63, 63, 58, 57, 51, 11, 23,  4, 27, 38, 51,  4, 19, 10,\n",
      "        58, 42, 23,  4, 19, 23, 65, 10,  4, 11, 44, 17,  3,  9, 34,  4,  5,  5,\n",
      "        17, 19, 18, 11,  2,  7, 47, 18,  8, 23,  6, 28,  3, 33,  5,  6, 12, 57,\n",
      "        12,  3, 10, 17, 12, 18, 15, 15,  3, 28,  8, 23, 37, 28, 43,  8, 23, 19,\n",
      "        18, 26, 16, 18, 17,  7,  9, 31,  2,  3, 24,  8,  6,  7,  7, 53,  6,  5,\n",
      "         9, 16,  4, 18,  7, 15, 28,  2,  8,  5,  7,  3,  9, 32,  6,  5,  9, 18,\n",
      "        47,  7, 14, 19,  2, 15,  5, 42,  4, 10, 10,  6, 18, 12,  7,  3,  2,  5,\n",
      "        12,  6, 43, 14,  7,  8,  2, 11,  5,  2, 18, 19, 12, 19, 43, 32, 10, 18,\n",
      "         8, 14, 28, 27,  8, 32,  9,  5,  5,  3,  4,  8, 27, 13,  8,  7, 54, 32,\n",
      "        15, 19, 47,  7,  5, 11, 10, 16, 16, 32,  8,  5,  4, 25, 54, 20,  7,  8,\n",
      "        44, 19, 16,  2,  7, 30, 12,  7, 53, 47, 33, 16, 13, 15, 10, 11,  6, 10,\n",
      "        45, 10, 45, 10, 26, 28, 37,  8, 10,  2,  3, 23, 19, 20,  2,  5, 14, 20,\n",
      "         9,  2, 16, 23,  6,  8, 14, 17,  6,  6,  3, 14, 19,  2, 15,  4, 25, 16,\n",
      "         8, 23, 13, 25,  4, 19, 18, 18,  7,  2, 16, 10,  8, 37, 32,  4, 14, 47,\n",
      "         8, 10, 28,  7, 11,  2,  6, 10, 13, 12, 15,  2, 39, 48,  6, 14, 21, 28,\n",
      "        11, 12,  3, 11, 44,  9, 22, 19, 28,  6,  3, 37,  8,  7, 45, 31, 13, 17,\n",
      "        13, 12,  7,  3, 11,  6,  4,  2, 13, 22,  8, 16, 15, 18,  6, 12,  4,  2,\n",
      "        31, 18,  7, 10,  6, 37, 30, 15, 47, 12,  3, 18, 12,  6,  3, 19,  9,  5,\n",
      "         7, 18,  2, 16, 14, 36,  3, 26, 15, 15,  2,  2,  5,  3, 19, 18, 18, 19,\n",
      "         8,  7, 15, 30,  6, 22, 28, 30, 32,  5,  4,  7,  9,  5,  2, 62],\n",
      "       device='cuda:0'),\n",
      "\t('seqlen',)), NamedTensor(\n",
      "\ttensor([10, 10, 40, 53, 48, 26, 63, 43, 36, 17, 26, 19, 47, 23, 26, 43, 50, 16,\n",
      "        43, 48, 27, 30, 42, 46,  5, 30, 20, 50, 37, 19,  9, 63, 16, 44, 22, 33,\n",
      "        46,  9, 64,  4, 60, 56, 58, 57, 51, 30, 40,  4, 27, 59, 51,  4, 19, 10,\n",
      "         7, 42, 30,  4, 24, 23, 65, 10,  4, 11, 52, 17, 25,  9, 34,  4, 21, 12,\n",
      "        17, 24, 56, 30, 34,  7, 51, 18,  8, 23, 22, 28,  3, 33, 29, 22, 12, 57,\n",
      "        21, 25, 10, 26, 12, 18, 15, 15,  3, 47,  8, 23, 27, 48, 43,  8, 23, 24,\n",
      "        20, 26, 53, 20, 26,  7, 31, 31,  2,  3, 19,  8,  6, 13,  7, 53,  6, 12,\n",
      "        31, 55,  4, 20, 58, 15, 28, 38,  8, 21,  7,  3,  9, 32, 22, 12,  9, 20,\n",
      "        45, 13, 14, 19,  2, 52,  5,  4, 42, 10, 10,  6, 18, 12,  7,  3,  2, 29,\n",
      "        12,  6, 43, 14,  7, 14,  2, 11, 21, 59, 20, 24, 12, 19, 43, 32, 10, 18,\n",
      "         8,  8, 28, 37,  8, 32,  9,  5,  5,  3,  4,  8, 27,  7,  8,  7, 54, 32,\n",
      "        15, 19, 47, 13, 12, 40, 10, 16, 16, 32,  8, 21,  4, 25, 57, 18, 58,  8,\n",
      "        44, 19, 16,  2, 13, 30, 12,  7, 53, 47, 33, 16, 13, 15, 10, 30,  6, 10,\n",
      "        45, 10, 45, 10, 26, 48, 37,  8, 10, 34,  3, 23, 19, 20,  2, 21, 14, 20,\n",
      "         9, 38, 16, 30,  6, 14,  8, 17,  6,  6, 25,  8, 24, 35, 15,  4,  3, 53,\n",
      "         8, 23, 13, 25, 42, 24, 18, 18,  7, 35, 16, 10,  8, 37, 32,  4, 14, 48,\n",
      "         8, 10, 45,  7, 11,  2,  6, 10, 13, 29, 44, 38, 39, 47, 22,  8, 21, 47,\n",
      "        23, 12,  3, 30, 15,  9, 22, 19, 28, 22,  3, 37,  8,  7, 51, 31,  7, 17,\n",
      "         7, 12,  7,  3, 11, 22, 42, 39, 13, 22, 14, 55, 15, 18,  6, 12, 42,  2,\n",
      "        31, 18, 13, 10,  6, 37, 23, 44, 46, 12, 25, 20, 29, 22,  3, 24,  9, 12,\n",
      "         7, 18, 34, 55,  8, 36,  3, 17, 44, 15,  2,  2, 12,  3, 19, 18, 20, 19,\n",
      "         8, 13, 44, 23,  6, 22, 28, 11, 32, 12,  4, 13,  9,  5, 34, 65],\n",
      "       device='cuda:0'),\n",
      "\t('seqlen',)), 0.6018471870595086)\n",
      "Test accuracy:  (NamedTensor(\n",
      "\ttensor([10, 50, 52,  ..., 24, 39, 62], device='cuda:0'),\n",
      "\t('seqlen',)), NamedTensor(\n",
      "\ttensor([10, 50, 15,  ..., 19,  2, 65], device='cuda:0'),\n",
      "\t('seqlen',)), 0.5774748489727727)\n"
     ]
    }
   ],
   "source": [
    "test_ac, train_ac = (accuracy(test_iter_bucket, model, aa_compress, teacher_force=1), \n",
    "                     accuracy(train_iter_bucket, model, aa_compress, teacher_force=1))\n",
    "print(\"Train accuracy: \", train_ac)\n",
    "print(\"Test accuracy: \", test_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "qIJAsVueeMS2",
    "outputId": "17b06c02-f6bb-4479-aefa-a8e91a84b768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train PPL:  3.545165146657026\n",
      "Test PPL:  3.8990431628178577\n"
     ]
    }
   ],
   "source": [
    "test_ppl, train_ppl = (PPL(test_iter_bucket, model, aa_compress, teacher_force=1), \n",
    "                     PPL(train_iter_bucket, model, aa_compress, teacher_force=1))\n",
    "print(\"Train PPL: \", train_ppl)\n",
    "print(\"Test PPL: \", test_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pvZ8ml3B3FLH"
   },
   "outputs": [],
   "source": [
    "test_lstm_predicts, test_target_vals, lstm_test_ac = output_predictions_and_target(test_iter_bucket, model, aa_compress, teacher_force = 0)\n",
    "# train_unigram_predicts, train_target_vals, unigram_train_ac = prediction_mismatches(test_iter_bucket, model, aa_compress)\n",
    "\n",
    "# unigram_test_ac = accuracy(test_iter_bucket, model, aa_compress)\n",
    "\n",
    "# print(\"Train accuracy: \", unigram_train_ac)\n",
    "# print(\"Test accuracy: \", test_lstm_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rZ1_3mpXkYQ-",
    "outputId": "d439f229-1b91-4c69-b894-91ef33a95c1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.552258961673331"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_test_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Pxb6nz5jOpn"
   },
   "outputs": [],
   "source": [
    "def convert_output_to_DNA(predicts):\n",
    "  seqs = []\n",
    "  \n",
    "  for pred in predicts:\n",
    "    seq = []\n",
    "    for j in range(len(pred)):\n",
    "      seq.append(TEXT.vocab.itos[np.array(pred.values)[j]])\n",
    "    \n",
    "    dna_seq = \"\".join(seq).upper()\n",
    "    seqs.append(dna_seq)\n",
    "    \n",
    "  return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1y718HoFqUzB"
   },
   "outputs": [],
   "source": [
    "output_seqs = convert_output_to_DNA(test_lstm_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9muwRApqYMT"
   },
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqUtils import CodonUsage\n",
    "from Bio.SeqUtils import IUPACData\n",
    "\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "30TixYcprCWn"
   },
   "outputs": [],
   "source": [
    "def count_codons(DNA_seq):\n",
    "  ''' Calculate the counts of each codon given a CDS\n",
    "  \n",
    "  Args:\n",
    "    dna sequence in indexable form\n",
    "    \n",
    "  Returns:\n",
    "    dict{str, int}: dictionary with codons as key and corresponding number of occurences\n",
    "  \n",
    "  '''\n",
    "  \n",
    "  codons_dict = CodonUsage.CodonsDict.copy()\n",
    "  for codon_start in range(0, len(DNA_seq), 3):\n",
    "    codons_dict[str(DNA_seq[codon_start:codon_start+3])] += 1\n",
    "   \n",
    "  return codons_dict\n",
    "\n",
    "\n",
    "def count_codons_fasta(fasta_file):\n",
    "  ''' Calculate the counts of each codon given a set of CDS\n",
    "  \n",
    "  Args:\n",
    "    Fasta file \n",
    "    \n",
    "  Returns:\n",
    "    dict{str, int}: dictionary with codons as key and corresponding number of occurences\n",
    "  \n",
    "  '''\n",
    "  \n",
    "  codons_dict = CodonUsage.CodonsDict.copy()\n",
    "  \n",
    "  for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "    seq = record.seq\n",
    "    if len(seq) % 3 != 0:\n",
    "      continue\n",
    "      \n",
    "    #count the codons for this sequence\n",
    "    for codon_start in range(0, len(seq), 3):\n",
    "      codons_dict[str(seq[codon_start:codon_start+3])] += 1\n",
    "   \n",
    "  return codons_dict\n",
    "\n",
    "def count_codons_list(seqs):\n",
    "  ''' Calculate the counts of each codon given list of seqs\n",
    "  \n",
    "  Args:\n",
    "    Fasta file \n",
    "    \n",
    "  Returns:\n",
    "    dict{str, int}: dictionary with codons as key and corresponding number of occurences\n",
    "  \n",
    "  '''\n",
    "  \n",
    "  codons_dict = CodonUsage.CodonsDict.copy()\n",
    "  \n",
    "  for seq in seqs:\n",
    "    if len(seq) % 3 != 0:\n",
    "      continue\n",
    "      \n",
    "    #count the codons for this sequence\n",
    "    for codon_start in range(0, len(seq), 3):\n",
    "      codons_dict[str(seq[codon_start:codon_start+3])] += 1\n",
    "   \n",
    "  return codons_dict\n",
    "\n",
    "def calculate_codon_frequency(codon_counts):\n",
    "  ''' Calculate the counts of each codon given a set of CDS\n",
    "  \n",
    "  Args:\n",
    "    codon usage table\n",
    "    \n",
    "  Returns:\n",
    "    dict{str, float}: dictionary with codons as key and corresponding \n",
    "    frequency of codon for AA\n",
    "  \n",
    "  '''\n",
    "  codon_freqs = CodonUsage.CodonsDict.copy()\n",
    "  \n",
    "  for _, synonymous_codons in CodonUsage.SynonymousCodons.items():\n",
    "    total_AA_count = sum([codon_counts[codon] for codon in synonymous_codons])\n",
    "    \n",
    "    if total_AA_count == 0:\n",
    "      continue\n",
    "      \n",
    "    for codon in synonymous_codons:\n",
    "      codon_freqs[codon] = codon_counts[codon] / total_AA_count\n",
    "  \n",
    "  return codon_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "140bC_2grJON"
   },
   "outputs": [],
   "source": [
    "lstm_codon_table = count_codons_list(output_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dcYYUaBg7atW",
    "outputId": "cbb2ceea-a81e-4ae2-e62a-a21a35689708"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387259"
      ]
     },
     "execution_count": 115,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([lstm_codon_table[key] for key in lstm_codon_table])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1105
    },
    "colab_type": "code",
    "id": "OlCzxMVqsnGJ",
    "outputId": "bb004e2a-1584-41eb-f147-8c1c37e8ab15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AAA': 0.9963053487182402,\n",
       " 'AAC': 0.6207480776749642,\n",
       " 'AAG': 0.003694651281759791,\n",
       " 'AAT': 0.37925192232503585,\n",
       " 'ACA': 0.0,\n",
       " 'ACC': 0.764034253092293,\n",
       " 'ACG': 0.23372978116079923,\n",
       " 'ACT': 0.002235965746907707,\n",
       " 'AGA': 0.00022966331358228838,\n",
       " 'AGC': 0.7082299690995343,\n",
       " 'AGG': 4.593266271645767e-05,\n",
       " 'AGT': 0.004439221830526178,\n",
       " 'ATA': 8.149295085975064e-05,\n",
       " 'ATC': 0.2722679488224269,\n",
       " 'ATG': 1.0,\n",
       " 'ATT': 0.7276505582267134,\n",
       " 'CAA': 0.27123995407577495,\n",
       " 'CAC': 0.2496993549797748,\n",
       " 'CAG': 0.728760045924225,\n",
       " 'CAT': 0.7503006450202252,\n",
       " 'CCA': 0.028163358953768498,\n",
       " 'CCC': 0.01238958357232993,\n",
       " 'CCG': 0.8962372375817368,\n",
       " 'CCT': 0.06320981989216473,\n",
       " 'CGA': 0.0,\n",
       " 'CGC': 0.595241376142575,\n",
       " 'CGG': 0.02388498461255799,\n",
       " 'CGT': 0.3805980432685683,\n",
       " 'CTA': 0.0,\n",
       " 'CTC': 0.06438978901969926,\n",
       " 'CTG': 0.9013404825737266,\n",
       " 'CTT': 0.01997901853362863,\n",
       " 'GAA': 0.8579420315390714,\n",
       " 'GAC': 0.14609145643628402,\n",
       " 'GAG': 0.14205796846092855,\n",
       " 'GAT': 0.8539085435637159,\n",
       " 'GCA': 0.07225940089228808,\n",
       " 'GCC': 0.28630231570002124,\n",
       " 'GCG': 0.6170331421287444,\n",
       " 'GCT': 0.02440514127894625,\n",
       " 'GGA': 0.016008537886873,\n",
       " 'GGC': 0.48724481013529797,\n",
       " 'GGG': 0.11323028195682859,\n",
       " 'GGT': 0.3835163700210005,\n",
       " 'GTA': 0.00819093348397119,\n",
       " 'GTC': 0.1454596808360401,\n",
       " 'GTG': 0.6436943934472532,\n",
       " 'GTT': 0.20265499223273548,\n",
       " 'TAA': 0.9485238455715367,\n",
       " 'TAC': 0.6196668744989757,\n",
       " 'TAG': 0.0,\n",
       " 'TAT': 0.3803331255010243,\n",
       " 'TCA': 0.012099055577316447,\n",
       " 'TCC': 0.06593550071810941,\n",
       " 'TCG': 0.15019367193280236,\n",
       " 'TCT': 0.05910258084171128,\n",
       " 'TGA': 0.05147615442846329,\n",
       " 'TGC': 0.7966386554621848,\n",
       " 'TGG': 1.0,\n",
       " 'TGT': 0.20336134453781513,\n",
       " 'TTA': 0.00300734351322998,\n",
       " 'TTC': 0.2974137931034483,\n",
       " 'TTG': 0.011283366359715585,\n",
       " 'TTT': 0.7025862068965517}"
      ]
     },
     "execution_count": 81,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_codon_frequency(lstm_codon_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "u7ZyaPxQq695",
    "outputId": "424b2ac1-d919-4def-d06c-40da595374f7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-e3c8840d83e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_output_to_DNA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_target_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_output_to_DNA' is not defined"
     ]
    }
   ],
   "source": [
    "target_seqs = convert_output_to_DNA(test_target_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4RtL3VZq7HQ"
   },
   "outputs": [],
   "source": [
    "target_codon_table = count_codons_list(target_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1105
    },
    "colab_type": "code",
    "id": "if3BFQvTtIvo",
    "outputId": "4e245094-0b47-4ef9-d449-0ef4f81f92e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AAA': 0.7706474165861422,\n",
       " 'AAC': 0.5516095399452626,\n",
       " 'AAG': 0.22935258341385778,\n",
       " 'AAT': 0.44839046005473737,\n",
       " 'ACA': 0.1317316841103711,\n",
       " 'ACC': 0.4326831588962892,\n",
       " 'ACG': 0.26622264509990484,\n",
       " 'ACT': 0.16936251189343482,\n",
       " 'AGA': 0.04000734922603463,\n",
       " 'AGC': 0.2738390564477521,\n",
       " 'AGG': 0.020210371595241375,\n",
       " 'AGT': 0.15162989076032554,\n",
       " 'ATA': 0.0764403879064461,\n",
       " 'ATC': 0.4155325564338685,\n",
       " 'ATG': 1.0,\n",
       " 'ATT': 0.5080270556596854,\n",
       " 'CAA': 0.344833524684271,\n",
       " 'CAC': 0.4261506504864983,\n",
       " 'CAG': 0.655166475315729,\n",
       " 'CAT': 0.5738493495135017,\n",
       " 'CCA': 0.19215326373752437,\n",
       " 'CCC': 0.12395319490650453,\n",
       " 'CCG': 0.5248938855110703,\n",
       " 'CCT': 0.15899965584490078,\n",
       " 'CGA': 0.06651049561343071,\n",
       " 'CGC': 0.3980524551008222,\n",
       " 'CGG': 0.09696385099444214,\n",
       " 'CGT': 0.3782554774700289,\n",
       " 'CTA': 0.03527217624431752,\n",
       " 'CTC': 0.10404476046159226,\n",
       " 'CTG': 0.49609511598088357,\n",
       " 'CTT': 0.10257605781559623,\n",
       " 'GAA': 0.6876046163333627,\n",
       " 'GAC': 0.3674677812608847,\n",
       " 'GAG': 0.3123953836666373,\n",
       " 'GAT': 0.6325322187391152,\n",
       " 'GCA': 0.21282132993414063,\n",
       " 'GCC': 0.270501380922031,\n",
       " 'GCG': 0.35256001699596345,\n",
       " 'GCT': 0.1641172721478649,\n",
       " 'GGA': 0.10865149585155094,\n",
       " 'GGC': 0.40255448066926014,\n",
       " 'GGG': 0.15068681791579164,\n",
       " 'GGT': 0.33810720556339724,\n",
       " 'GTA': 0.15651037988984606,\n",
       " 'GTC': 0.21416466600762604,\n",
       " 'GTG': 0.3677093630843101,\n",
       " 'GTT': 0.26161559101821774,\n",
       " 'TAA': 0.6351249053747161,\n",
       " 'TAC': 0.42950031174846354,\n",
       " 'TAG': 0.07418622255866768,\n",
       " 'TAT': 0.5704996882515365,\n",
       " 'TCA': 0.12373242808025417,\n",
       " 'TCC': 0.1488009748879314,\n",
       " 'TCG': 0.15363189276232755,\n",
       " 'TCT': 0.14836575706140923,\n",
       " 'TGA': 0.2906888720666162,\n",
       " 'TGC': 0.553781512605042,\n",
       " 'TGG': 1.0,\n",
       " 'TGT': 0.446218487394958,\n",
       " 'TTA': 0.1305979717915841,\n",
       " 'TTC': 0.43268255578093306,\n",
       " 'TTG': 0.13141391770602634,\n",
       " 'TTT': 0.5673174442190669}"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_codon_frequency(target_codon_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sD9bhwKnq7BV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWbAKkUbq7EL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "WBNaUy8u3NI9",
    "outputId": "4eb017c8-815a-40db-9609-0e02d383b14c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = mismatched_indices(test_lstm_predicts, test_target_vals)\n",
    "mismatch_over_sequence = pool_mismatches(out[0], out[1])\n",
    "plt.scatter(range(len(mismatch_over_sequence))[:300], mismatch_over_sequence[:300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7z9yxiwXOxwp"
   },
   "source": [
    "## Model inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6MmepHwgStM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3JuDtYVygSwP"
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "batch = next(iter(train_iter_bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aBzEPjeze1Z4"
   },
   "outputs": [],
   "source": [
    "seqlen = batch.sequence.shape['seqlen']\n",
    "src = batch.sequence.narrow('seqlen',0,seqlen-1)\n",
    "trg = batch.sequence.narrow('seqlen', 1,seqlen-1)\n",
    "aa_compress(trg)\n",
    "model(src, aa_compress(trg)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fPtcnIg38QO"
   },
   "outputs": [],
   "source": [
    "output = model(src, aa_compress(trg))\n",
    "mask_vec = ntorch.tensor(mask_tbl[trg.values], \n",
    "                         names=(\"seqlen\", \"batch\", \"vocablen\")).float()\n",
    "output = (mask_vec + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwUo499Z_2PU"
   },
   "outputs": [],
   "source": [
    "preds = get_prediction(batch, model, aa_compress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TuPTOeAmIGM4"
   },
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vRL7mrsaIqTx"
   },
   "outputs": [],
   "source": [
    "def translate_to_seq(x): \n",
    "  ''' Takes in single tensor of name seqlen'''\n",
    "  my_str = \"\".join([TEXT.vocab.itos[i] for i in x.values])\n",
    "  my_str = my_str.split(\"<pad>\")[0]\n",
    "  my_str = my_str.split(\"<unk>\")[0]\n",
    "\n",
    "  if \"<start>\" in my_str: \n",
    "    my_str = my_str.split(\"<start>\")[1]\n",
    "    \n",
    "    \n",
    "  \n",
    "  return my_str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvrZXdREIGRS"
   },
   "outputs": [],
   "source": [
    "src_str = translate_to_seq(batch.sequence.get(\"batch\", 0))\n",
    "res_str = translate_to_seq(preds.get(\"batch\", 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jOwvHkXsIpNx"
   },
   "outputs": [],
   "source": [
    "Seq(src_str).translate(), Seq(res_str).translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fTE8PQsLjJX"
   },
   "outputs": [],
   "source": [
    "src_str_tok = np.array(tokenize(src_str))\n",
    "res_str_tok = np.array(tokenize(res_str))\n",
    "correct_locs =  (res_str_tok == src_str_tok)\n",
    "(np.array([str(Seq(i).translate()) for i in src_str_tok[~correct_locs]]),\n",
    "src_str_tok[~correct_locs], res_str_tok[~correct_locs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F0X3n6Q_Un5x"
   },
   "outputs": [],
   "source": [
    "## See relative frequencies of what we predict vs. what's actually used \n",
    "conv_to_prob = lambda x : np.array([eColi_codon_table[y] for y in x])\n",
    "print(\"Source frequencies of mispredicted codons:\\n\", \n",
    "      conv_to_prob(src_str_tok[~correct_locs]) )\n",
    "print(\"Native frequency of mispredicted codons:\\n\", \n",
    "      conv_to_prob(res_str_tok[~correct_locs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oitcGobbn2l5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7z4m1xTbCPZp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QKgVRU41CP2a"
   },
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OfNhgqcfCRIN"
   },
   "outputs": [],
   "source": [
    "def prediction_mismatches(data_iter, model, aa_compress, teacher_force=0):\n",
    "  '''\n",
    "  Produce a NUM_SEQUENCE x LEN_SEQUENCE matrix, boolean values. 1 if prediction is correct, 0 if incorrect\n",
    "  TODO: \n",
    "  - Mask all non synonymous codons in train\n",
    "    For now, we can do this at prediction time.. \n",
    "  - Precompute the text / target in the iterator function \n",
    "  ''' \n",
    "  \n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "  model.teacher_force_prob = teacher_force\n",
    "  aa_compress.to(device)\n",
    "  aa_compress.eval()\n",
    "  num_correct = 0 \n",
    "  num_total = 0 \n",
    "  \n",
    "  prediction_list = []\n",
    "  target_list = []\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    for i, batch in enumerate(data_iter):\n",
    "      \n",
    "      # Select for all non zero tensors\n",
    "      # Use this to find all indices that aren't padding\n",
    "      seq_len = batch.sequence.shape[\"seqlen\"]\n",
    "      text = batch.sequence.narrow(\"seqlen\", 0, seq_len - 1)\n",
    "      target = batch.sequence.narrow(\"seqlen\", 1, seq_len - 1)\n",
    "      \n",
    "      stacked_target = target.stack(dims=(\"batch\", \"seqlen\"), \n",
    "                                          name=\"seqlen\")\n",
    "      mask = (stacked_target != TEXT.vocab.stoi[\"<pad>\"])\n",
    "      prop_indices = (ntorch.nonzero(mask)\n",
    "                      .get(\"inputdims\", 0)\n",
    "                     ).rename(\"elements\", \"seqlen\")\n",
    "      # Forward\n",
    "      predictions = model(text, aa_compress(target))\n",
    "      # Mask all outputs that don't work\n",
    "      mask_bad_codons = ntorch.tensor(mask_tbl[target.values], \n",
    "                         names=(\"seqlen\", \"batch\", \"vocablen\")).float()\n",
    "      predictions = (mask_bad_codons.double() + predictions.double())\n",
    "      \n",
    "      \n",
    "     \n",
    "      # Stack the predictions into one long vector\n",
    "#       predictions = predictions.stack(dims=(\"batch\", \"seqlen\"), name=\"seqlen\")\n",
    "\n",
    "      predictions = predictions.argmax(\"vocablen\").index_select(\"seqlen\", \n",
    "                                                                prop_indices)\n",
    "      stacked_target = stacked_target.index_select(\"seqlen\", prop_indices)\n",
    "      \n",
    "#       prediction_list.append(predictions)\n",
    "#       target_list.append(stacked_target)\n",
    "      \n",
    "      num_correct += (predictions == stacked_target).sum().item()   \n",
    "      num_total += predictions.shape[\"seqlen\"]\n",
    "      \n",
    "      # For quick results, toggle this\n",
    "#       if i == 20: \n",
    "#         break\n",
    "      \n",
    "#   return num_correct / num_total \n",
    "\n",
    "  return (prediction_list, target_list, num_correct / num_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "usDAswEJHBhc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vdYGx7hPn3N3"
   },
   "source": [
    "## MISC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zSCQ7JLmo6bR"
   },
   "source": [
    "#### Positional encoding\n",
    "\n",
    "Sasha suggested that this doesn't  matter because RNN's should be able to count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "275u-tVRT6wn"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(ntorch.nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = ntorch.nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        # Keep in log space\n",
    "        div_term = (torch.arange(0, d_model, 2).float() *\n",
    "                             -(math.log(10000.0) / d_model)).exp()\n",
    "        \n",
    "\n",
    "        pe[:, 0::2] = (position * div_term).sin()\n",
    "        pe[:, 1::2] = (position * div_term[:int(d_model /2)]).cos()\n",
    "        pe = pe.unsqueeze(0)\n",
    "        pe = ntorch.tensor(pe, names=(\"batch\", \"seqlen\", \"hiddendim\"))\n",
    "        self.pe = pe.to(device)\n",
    "    def forward(self, embed_seq):\n",
    "        embed_seq = embed_seq.float() + self.pe[\n",
    "                {\"batch\": 0,\"seqlen\" : slice(0,embed_seq.shape[\"seqlen\"])}\n",
    "            ]\n",
    "        return self.dropout(embed_seq)\n",
    "\n",
    "# pe = PositionalEncoding(d_model=23, dropout=0.1).to(device)\n",
    "# print(batch.sequence.shape['seqlen'])\n",
    "# print(embed(batch.sequence).rename(\"onehot\", \"hiddendim\").shape)\n",
    "# pe(batch.sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fQuYrDgHUEAN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DownstreamTests_LanguageModel.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
